import os
import pandas as pd
import numpy as np
import wfdb
import ast
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support

import tensorflow as tf
from tensorflow.keras.layers import (Conv1D, BatchNormalization, Activation,
                                     MaxPooling1D, GlobalAveragePooling1D,
                                     Dense, Dropout, Input, SpatialDropout1D)
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical, Sequence
from tensorflow.keras import regularizers



PATH = '/Users/hamza/Desktop/–≠–ö–ì/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/Needit/'
SAMPLING_RATE = 100
SIGNAL_LENGTH = 1000
BATCH_SIZE = 32
EPOCHS = 50
BALANCE_MULTIPLIER = 1.3
MAX_BALANCED_SIZE = 4000
AUGMENT_FRACTION = 0.4
MAX_TIME_SHIFT = 20
AUGMENT_NOISE_STD = 0.01



# 1) –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
Y=pd.read_csv(PATH + 'ptbxl_database.csv', index_col='ecg_id')
Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x)) # ast.literal_eval –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç "{'NORM': 100}" –≤ –Ω–∞—Å—Ç–æ—è—â–∏–π —Å–ª–æ–≤–∞—Ä—å Python
agg_df = pd.read_csv(PATH + 'scp_statements.csv', index_col= 0 )
agg_df = agg_df[agg_df.diagnostic_class.notnull()] # —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤ 5 –±–æ–ª—å—à–∏—Ö –≥—Ä—É–ø–ø






selected_classes = ['NORM', 'MI', 'STTC']  # –ù–æ—Ä–º–∞, –ò–Ω—Ñ–∞—Ä–∫—Ç, –ò–∑–º–µ–Ω–µ–Ω–∏—è ST-T

def aggregate_diagnostic(y_dic):
  #–í–º–µ—Å—Ç–æ –∫—É—á–∏ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–¥–æ–≤ –º—ã –ø–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫: ['MI']
  tmp=[]
  for key in y_dic.keys():
    if key in agg_df.index:
      diagnostic_class = agg_df.loc[key].diagnostic_class
      if diagnostic_class in selected_classes:
                tmp.append(diagnostic_class)
  return list(set(tmp))
Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic) #–ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫–æ –≤—Å–µ–º –ø–∞—Ü–∏–µ–Ω—Ç–∞–º.
Y['counts'] = Y.diagnostic_superclass.apply(len)
df_clean = Y[Y.counts == 1].copy() # Filter - –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å –û–î–ù–ò–ú –¥–∏–∞–≥–Ω–æ–∑–æ–º
df_clean['label'] = df_clean.diagnostic_superclass.apply(lambda x: x[0]) # –í—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–∞. –ë—ã–ª–æ ['MI'] (—Å–ø–∏—Å–æ–∫), —Å—Ç–∞–ª–æ 'MI' (—Å—Ç—Ä–æ–∫–∞)

# –ü—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ –±—É–∫–≤ –≤ —Ü–∏—Ñ—Ä—ã
le = LabelEncoder()
y_indices = le.fit_transform(df_clean['label'])
classes = le.classes_


print(" –ú–ê–ü–ü–ò–ù–ì –ö–õ–ê–°–°–û–í (MAPPING)")
for i, class_name in enumerate(classes):
    print(f"  {class_name} <---> {i}")

# –ß—Ç–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
def load_raw_data(df , sampling_rate, path) :
  if sampling_rate == 100 :
    data = [wfdb.rdsamp(path + f)[0] for f in df.filename_lr]
  else :
    data = [wfdb.rdsamp(path + f)[0] for f in df.filename_hr]
  return np.array(data)
X = load_raw_data(df_clean, SAMPLING_RATE, PATH)
print(" –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•")
print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ (Y): {len(Y)}")
print(f"–ü–æ—Å–ª–µ —á–∏—Å—Ç–∫–∏ (—Ç–æ–ª—å–∫–æ 1 –¥–∏–∞–≥–Ω–æ–∑): {len(df_clean)}")
print(f"–ú—ã –±–µ—Ä–µ–º (sample count): {X.shape[0]} –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤")
print(f"–î–ª–∏–Ω–∞ —Å–∏–≥–Ω–∞–ª–∞: {X.shape[1]} —Ç–æ—á–µ–∫ (10 —Å–µ–∫—É–Ω–¥)")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤: {X.shape[2]} (I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6)")
print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏: {X.nbytes / (1024**3):.2f} GB")
print("="*30 + "\n")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
def plot_single_ecg_signal(signal, label, sample_rate=100, save_path=None):
    time = np.arange(signal.shape[0]) / sample_rate
    leads = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
    fig, axes = plt.subplots(4, 3, figsize=(15, 12))
    fig.suptitle(f'12-–∫–∞–Ω–∞–ª—å–Ω–∞—è –≠–ö–ì - –î–∏–∞–≥–Ω–æ–∑: {label}', fontsize=16, fontweight='bold')
    for ax, lead, channel in zip(axes.flat, leads, signal.T):
        ax.plot(time, channel, linewidth=1.0, color='blue')
        ax.set_title(f'–û—Ç–≤–µ–¥–µ–Ω–∏–µ {lead}', fontweight='bold')
        ax.set_xlabel('–í—Ä–µ–º—è (—Å–µ–∫)')
        ax.set_ylabel('–ê–º–ø–ª–∏—Ç—É–¥–∞')
        ax.grid(True, alpha=0.3)
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


def plot_random_sample():
    idx = np.random.randint(0, len(X))
    signal = X[idx]
    label_text = df_clean.iloc[idx]['label']
    plot_single_ecg_signal(signal, label_text, sample_rate=SAMPLING_RATE,
                           save_path=os.path.join(PATH, 'single_ecg_detail.png'))


print("–†–∏—Å—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Å–∏–≥–Ω–∞–ª...")
plot_random_sample()


def augment_signal(signal):
    augmented = np.copy(signal)
    if MAX_TIME_SHIFT > 0:
        shift = np.random.randint(-MAX_TIME_SHIFT, MAX_TIME_SHIFT + 1)
        augmented = np.roll(augmented, shift, axis=0)
    scale = np.random.uniform(0.9, 1.1)
    augmented *= scale
    noise = np.random.normal(0, AUGMENT_NOISE_STD, augmented.shape)
    augmented += noise
    return augmented.astype(np.float32)


def augment_batch(signals):
    return np.stack([augment_signal(sig) for sig in signals])


# 2) –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö 
X_train_raw, X_test, y_train_raw, y_test = train_test_split(
    X, y_indices, test_size=0.15, random_state=42, stratify=y_indices)
X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(
    X_train_raw, y_train_raw, test_size=0.1, random_state=42, stratify=y_train_raw)
print("–†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
print(f"Train (raw): {X_train_raw.shape} | –†–∞–∑–º–µ—Ä: {X_train_raw.nbytes / (1024**3):.2f} GB")
print(f"Validation (raw): {X_val_raw.shape} | –†–∞–∑–º–µ—Ä: {X_val_raw.nbytes / (1024**3):.2f} GB")
print(f"Test: {X_test.shape} | –†–∞–∑–º–µ—Ä: {X_test.nbytes / (1024**3):.2f} GB")
print("="*30 + "\n")

# 3) –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ (Upsampling)
def plot_pie_distribution(y_data, title):
    unique, counts = np.unique(y_data, return_counts=True)
    labels = [classes[i] for i in unique]
    plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140, colors=plt.cm.Pastel1.colors)
    plt.title(title)

#  –ì—Ä–∞—Ñ–∏–∫ –î–û –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plot_pie_distribution(y_train_raw, "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –î–û –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (Train)")


# –ü—Ä–æ—Ü–µ—Å—Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
# –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
train_df = pd.DataFrame({'label_idx': y_train_raw})
train_df['original_idx'] = range(len(train_df))

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –î–û –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
print("–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í –î–û –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò:")
class_counts_before = train_df['label_idx'].value_counts().sort_index()
for class_idx, count in class_counts_before.items():
    print(f"  –ö–ª–∞—Å—Å {class_idx} ({classes[class_idx]}): {count} –æ–±—Ä–∞–∑—Ü–æ–≤")
print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {class_counts_before.min()}")
print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {class_counts_before.max()}")
print(f"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Å–∞: {class_counts_before.mean():.0f}")

# –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
class_counts = train_df['label_idx'].value_counts()
min_count = class_counts.min()
target_size = min(int(min_count * BALANCE_MULTIPLIER), MAX_BALANCED_SIZE)
target_size = max(target_size, min_count)
print(f"\n–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞: min={min_count}, target={target_size}")

balanced_indices = []
for class_idx in range(len(classes)):
    class_data = train_df[train_df['label_idx'] == class_idx]
    current_size = len(class_data)
    replace = current_size < target_size
    data_resampled = resample(
        class_data,
        replace=replace,
        n_samples=target_size,
        random_state=42
    )
    balanced_indices.extend(data_resampled['original_idx'].values)

X_train_balanced = X_train_raw[balanced_indices]
y_train_balanced = y_train_raw[balanced_indices]

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ü–û–°–õ–ï –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
print("\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í –ü–û–°–õ–ï –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò:")
class_counts_after = pd.Series(y_train_balanced).value_counts().sort_index()
for class_idx, count in class_counts_after.items():
    print(f"  –ö–ª–∞—Å—Å {class_idx} ({classes[class_idx]}): {count} –æ–±—Ä–∞–∑—Ü–æ–≤")

# –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
shuffle_idx = np.random.permutation(len(X_train_balanced))
X_train = X_train_balanced[shuffle_idx]
y_train = y_train_balanced[shuffle_idx]

if AUGMENT_FRACTION > 0:
    augment_size = int(len(X_train) * AUGMENT_FRACTION)
    if augment_size > 0:
        sample_indices = np.random.choice(len(X_train), size=augment_size, replace=False)
        X_aug = augment_batch(X_train[sample_indices])
        y_aug = y_train[sample_indices]
        X_train = np.concatenate([X_train, X_aug], axis=0)
        y_train = np.concatenate([y_train, y_aug], axis=0)
        shuffle_idx = np.random.permutation(len(X_train))
        X_train = X_train[shuffle_idx]
        y_train = y_train[shuffle_idx]
        print(f"üîÅ –î–æ–±–∞–≤–ª–µ–Ω–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {augment_size}. –ù–æ–≤—ã–π train —Ä–∞–∑–º–µ—Ä: {len(X_train)}")

#  –ì—Ä–∞—Ñ–∏–∫ –ü–û–°–õ–ï –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
plt.subplot(1, 2, 2)
plot_pie_distribution(y_train, "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ü–û–°–õ–ï –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ (Train)")
plt.savefig('class_distribution_pie.png')
plt.show()

print(f"\nüìä –†–ê–ó–ú–ï–†–´ –î–ê–ù–ù–´–• –ü–û–°–õ–ï –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò:")
print(f"Train –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {X_train.shape}")
print(f"–†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: {X_train.nbytes / (1024**3):.2f} GB")
print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_train)}")
print(f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: {len(X_train) / len(X_train_raw):.2f}x")
print("="*30 + "\n")

# One-Hot Encoding
y_train_cat = to_categorical(y_train, num_classes=len(classes)) #–ë—ã–ª–æ: [0, 1, 2] (–≥–¥–µ 0=–Ω–æ—Ä–º–∞, 1=–∏–Ω—Ñ–∞—Ä–∫—Ç, 2=ST-T) –°—Ç–∞–ª–æ: [[1,0,0], [0,1,0], [0,0,1]]
y_val_cat = to_categorical(y_val_raw, num_classes=len(classes))
y_test_cat = to_categorical(y_test, num_classes=len(classes))
X_val = X_val_raw.copy()

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (z-score normalization)
#Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: (–∑–Ω–∞—á–µ–Ω–∏–µ - —Å—Ä–µ–¥–Ω–µ–µ) / —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
scaler=StandardScaler()
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_val_flat = X_val.reshape(X_val.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)
scaler.fit(X_train_flat) # fit —Ç–æ–ª—å–∫–æ –Ω–∞ Train:  –û–ë–£–ß–ê–ï–ú–°–Ø —Ç–æ–ª—å–∫–æ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!
X_train = scaler.transform(X_train_flat).reshape(X_train.shape)
X_val = scaler.transform(X_val_flat).reshape(X_val.shape)
X_test = scaler.transform(X_test_flat).reshape(X_test.shape)

# Build the model
def build_model (input_shape, num_classes):
    reg = regularizers.l2(1e-4)
    inputs = Input(shape=input_shape)

    x = Conv1D(32, 7, padding='same', use_bias=False, kernel_regularizer=reg)(inputs) # –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É—á–∞—Å—Ç–∫–∏ –ø–æ 7 –æ—Ç—Å—á–µ—Ç–æ–≤ (70 –º—Å)
    x = BatchNormalization()(x) # # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏
    x = Activation('relu')(x)
    x = MaxPooling1D(2)(x) ## –£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≤ 2 —Ä–∞–∑–∞: 1000 ‚Üí 500 [500, 32]
    x = SpatialDropout1D(0.1)(x)

    x = Conv1D(64, 5, padding='same', use_bias=False, kernel_regularizer=reg)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling1D(2)(x) # [500√ó64]
    x = Dropout(0.2)(x)

    x = Conv1D(128, 5, padding='same', use_bias=False, kernel_regularizer=reg)(x) # –∏—â–µ—Ç –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (QRS –∫–æ–º–ø–ª–µ–∫—Å—ã, ST —Å–µ–≥–º–µ–Ω—Ç—ã)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling1D(2)(x) #  [500√ó128]
    x = Dropout(0.3)(x)

    x = Conv1D(192, 3, padding='same', use_bias=False, kernel_regularizer=reg)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling1D()(x) # # [250√ó192] ‚Üí [192] –£–º–µ–Ω—å—à–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

    x = Dense(96, activation='relu', kernel_regularizer=reg)(x)
    x = Dropout(0.4)(x)
    x = Dense(48, activation='relu', kernel_regularizer=reg)(x)
    x = Dropout(0.3)(x) # –°–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è - –≤—ã–∫–ª—é—á–∞–µ–º 40% –Ω–µ–π—Ä–æ–Ω–æ–≤

    outputs = Dense(num_classes, activation='softmax')(x)
    return tf.keras.Model(inputs, outputs)

model = build_model((SIGNAL_LENGTH, 12), len(classes))

print("\n" + "="*50)
print("üèóÔ∏è  –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò (MODEL ARCHITECTURE)")
print("="*50)
model.summary()
print("="*50 + "\n")

model.compile(optimizer=Adam(learning_rate=5e-4),
              loss='categorical_crossentropy', #–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
              metrics=['accuracy'])

print("‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
print(f"–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam (learning_rate=0.0005)")
print(f"–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: categorical_crossentropy")
print(f"–ú–µ—Ç—Ä–∏–∫–∏: accuracy")
print("="*50 + "\n")

# ==========================================
# –®–ê–ì 5: –û–ë–£–ß–ï–ù–ò–ï (–ë–ï–ó CLASS_WEIGHT)
# ==========================================
# –°–æ–∑–¥–∞–µ–º callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
class ValidationF1Callback(tf.keras.callbacks.Callback):
    def __init__(self, validation_data, class_names):
        super().__init__()
        self.validation_data = validation_data # (X_val, y_val_cat)
        self.class_names = class_names # ['NORM', 'MI', 'STTC']

    def on_epoch_end(self, epoch, logs=None): #–í—ã–∑–æ–≤ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
        X_val, y_val = self.validation_data
        preds = self.model.predict(X_val, verbose=0) # # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        y_pred = np.argmax(preds, axis=1)
        y_true = np.argmax(y_val, axis=1)
        _, _, f1_scores, _ = precision_recall_fscore_support(
            y_true, y_pred, average=None, labels=range(len(self.class_names)))
        scores = {f'f1_{cls}': float(score) for cls, score in zip(self.class_names, f1_scores)}
        print(f"\nValidation F1 per class: {scores}")


callbacks = [
    ModelCheckpoint(
        filepath=os.path.join(PATH, 'best_model.h5'),
        monitor='val_loss',
        save_best_only=True,
        verbose=1,
        mode='min'
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=8,
        min_delta=1e-3,
        restore_best_weights=True,
        verbose=1,
        mode='min'
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.3,
        patience=3,
        verbose=1,
        mode='min',
        min_lr=1e-6
    ),
    CSVLogger('training_log.csv', append=False),
    ValidationF1Callback(validation_data=(X_val, y_val_cat), class_names=classes)
]

# –í–ê–ñ–ù–û: class_weight —É–±—Ä–∞–ª–∏, —Ç–∞–∫ –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã
history = model.fit(
    X_train, y_train_cat,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_val, y_val_cat),
    callbacks=callbacks,
    verbose=1
)

# ==========================================
# –®–ê–ì 6: –û–¶–ï–ù–ö–ê
# ==========================================
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test_cat, axis=1)

print("\nClassification Report:")
print(classification_report(y_true, y_pred_classes, target_names=classes))

# –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.title('Loss')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.title('Accuracy')
plt.legend()
plt.show()

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.title('Confusion Matrix (After Balancing)')
plt.show()

